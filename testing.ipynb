{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('data/bloomberg_quint_news.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING FOR MISSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting rows with missing values...\n",
      "Rows with missing values have been deleted.\n"
     ]
    }
   ],
   "source": [
    "def check_missing_values(df):\n",
    "    # Check for NaN, missing, or NaT values\n",
    "    missing_rows = df[df.isnull().any(axis=1)]\n",
    "\n",
    "    if not missing_rows.empty:\n",
    "        print(\"Deleting rows with missing values...\")\n",
    "        df.dropna(inplace=True)\n",
    "        print(\"Rows with missing values have been deleted.\")\n",
    "    else:\n",
    "        print(\"There are no missing values in the DataFrame.\")\n",
    "\n",
    "# Example usage:\n",
    "check_missing_values(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK resources (uncomment the following lines if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load pre-trained GloVe vectors\n",
    "glove_model = KeyedVectors.load_word2vec_format('gloves/glove.6B.300d.txt', binary=False)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_word_vector(word, model):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_closest_word(string, query, model):\n",
    "    # Preprocess the string and query\n",
    "    string_tokens = preprocess_text(string)\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    # Convert tokens into GloVe vectors\n",
    "    string_vectors = [get_word_vector(word, model) for word in string_tokens]\n",
    "    query_vector = get_word_vector(query_tokens[0], model)  # Assuming query consists of one word\n",
    "\n",
    "    # Calculate cosine similarity between query vector and each word vector in the string\n",
    "    similarities = [cosine_similarity([query_vector], [word_vector])[0][0] for word_vector in string_vectors]\n",
    "\n",
    "    # Find the index of the word in the string with the highest similarity to the query word\n",
    "    closest_word_index = np.argmax(similarities)\n",
    "\n",
    "    # Return the word in the string with the highest similarity to the query word\n",
    "    return string_tokens[closest_word_index]\n",
    "\n",
    "# Example usage:\n",
    "#string = \"This is an example string for NLP preprocessing.\"\n",
    "#query = \"example\"\n",
    "#closest_word = find_closest_word(string, query, glove_model)\n",
    "#print(\"Closest word in the string to the query word:\", closest_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  All You Need To Know Going Into Trade On Septe...   \n",
      "1  Bridgestone CEO Backs Safe Tokyo Olympics, Dia...   \n",
      "2  Stocks To Watch: HCL Tech, Cyient, M&M Financi...   \n",
      "3  Localised Lockdowns Cannot But Impinge On Econ...   \n",
      "4  CP Rail Wins Regulator Exemption From Tougher ...   \n",
      "5  Uber and Lyft Are Spending Millions on Driver ...   \n",
      "6  Stocks To Watch: Axis Bank, Bajaj Finance, Inf...   \n",
      "7  Japan to Approve Moderna Vaccine as Soon as Ma...   \n",
      "8                  Russia Doesn't Have a Navalny 2.0   \n",
      "9  All You Need To Know Going Into Trade On April 28   \n",
      "\n",
      "                                   short_description  \\\n",
      "0  Stocks in the news, big brokerage calls of the...   \n",
      "1  Bridgestone CEO Backs Safe Tokyo Olympics, Dia...   \n",
      "2     Here are the stocks to watch in trade today...   \n",
      "3  Localised Lockdowns Cannot But Impinge On Econ...   \n",
      "4  CP Rail Wins Regulator Exemption From Tougher ...   \n",
      "5  Uber and Lyft Are Spending Millions on Driver ...   \n",
      "6     Here are the stocks to watch in trade today...   \n",
      "7  Japan to Approve Moderna Vaccine as Soon as Ma...   \n",
      "8                  Russia Doesn't Have a Navalny 2.0   \n",
      "9  Stocks in the news, big brokerage calls of the...   \n",
      "\n",
      "                                         description  \n",
      "0  Asian stocks were steady early Thursday after ...  \n",
      "1   Bridgestone Corp. will support the Tokyo Olym...  \n",
      "2  Indian equity benchmarks reversed losses made ...  \n",
      "3  Nirmal Bang Report, We assess the ‘state of af...  \n",
      "4   Canadian Pacific Railway Ltd. won a petition ...  \n",
      "5   Misty Huffman has driven sporadically for Ube...  \n",
      "6  Indian equity benchmarks rose for a second ses...  \n",
      "7   , Japan’s government will approve the use of ...  \n",
      "8   Alexey Navalny long avoided the fate of Russi...  \n",
      "9  Asian stocks opened on a cautious note Wednesd...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK resources (uncomment the following lines if not already downloaded)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Load pre-trained GloVe vectors\n",
    "glove_model = KeyedVectors.load_word2vec_format('gloves/converted_vectors.txt', binary=False)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_word_vector(word, model):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_closest_word(string, query, model):\n",
    "    # Preprocess the string and query\n",
    "    string_tokens = preprocess_text(string)\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    # Convert tokens into GloVe vectors\n",
    "    string_vectors = [get_word_vector(word, model) for word in string_tokens]\n",
    "    query_vector = get_word_vector(query_tokens[0], model)  # Assuming query consists of one word\n",
    "\n",
    "    # Calculate cosine similarity between query vector and each word vector in the string\n",
    "    similarities = [cosine_similarity([query_vector], [word_vector])[0][0] for word_vector in string_vectors]\n",
    "\n",
    "    # Find the index of the word in the string with the highest similarity to the query word\n",
    "    closest_word_index = np.argmax(similarities)\n",
    "\n",
    "    # Return the word in the string with the highest similarity to the query word\n",
    "    return string_tokens[closest_word_index]\n",
    "\n",
    "def find_top_articles_with_keyword(data, keyword):\n",
    "    similarity_scores = []\n",
    "    for index, row in data.iterrows():\n",
    "        text = row['title'] + \" \" + row['short_description'] + \" \" + row['description']\n",
    "        similarity_score = find_similarity_with_keyword(text, keyword, glove_model)\n",
    "        similarity_scores.append((index, similarity_score))\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_articles = similarity_scores[:10]\n",
    "    top_article_indices = [article[0] for article in top_articles]\n",
    "    return data.iloc[top_article_indices]\n",
    "\n",
    "def find_similarity_with_keyword(text, keyword, model):\n",
    "    tokens = preprocess_text(text)\n",
    "    keyword_vector = get_word_vector(keyword, model)\n",
    "    if keyword_vector is None:\n",
    "        return 0  # Keyword not found in the model\n",
    "    text_vectors = [get_word_vector(word, model) for word in tokens]\n",
    "    text_vectors = [vec for vec in text_vectors if vec is not None]  # Remove None vectors\n",
    "    if len(text_vectors) == 0:\n",
    "        return 0  # No valid vectors found in the text\n",
    "    similarity_scores = [cosine_similarity([keyword_vector], [vec])[0][0] for vec in text_vectors]\n",
    "    return np.mean(similarity_scores)\n",
    "\n",
    "# Sample usage:\n",
    "keyword = \"covid\"  # Specify the keyword to search for\n",
    "top_articles = find_top_articles_with_keyword(data, keyword)\n",
    "print(top_articles[['title', 'short_description', 'description']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCH BASED ON KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  title  \\\n",
      "2400  Biden Says He Was Unaware of Giuliani Raid, Wo...   \n",
      "3187  ICICI Securities Q4 Review - Strong Customer A...   \n",
      "2478  DoorDash Goes on European Deal Hunt Just Month...   \n",
      "3182  Nestle India Q1 Review - Domestic Business Con...   \n",
      "292         Eastern Europe Feeds on a Shrinking Ukraine   \n",
      "386   Sony Sued for Limiting Purchases of Games to P...   \n",
      "4117  Penn Endowment Posts 41% Return, Buoyed by Sto...   \n",
      "104   Washington and Boston Are Beating NYC for Entr...   \n",
      "3310  Cox Agrees to Buy Enterprise Unit of EQT’s Fib...   \n",
      "2744  Oil Demand in India Drops as Wave of Virus Con...   \n",
      "\n",
      "                                      short_description  \\\n",
      "2400  Biden Says He Was Unaware of Giuliani Raid, Wo...   \n",
      "3187  ICICI Securities Q4 Review - Strong Customer A...   \n",
      "2478  DoorDash Goes on European Deal Hunt Just Month...   \n",
      "3182  Nestle India Q1 Review - Domestic Business Con...   \n",
      "292         Eastern Europe Feeds on a Shrinking Ukraine   \n",
      "386   Sony Sued for Limiting Purchases of Games to P...   \n",
      "4117  Penn Endowment Posts 41% Return, Buoyed by Sto...   \n",
      "104   Washington, Boston Beating NYC for Entrepreneu...   \n",
      "3310  Cox Agrees to Buy Enterprise Unit of EQT’s Fib...   \n",
      "2744  India’s Covid-19 crisis has pummeled demand fo...   \n",
      "\n",
      "                                            description  \n",
      "2400   President Joe Biden said he didn’t know ahead...  \n",
      "3187  Motilal Oswal Report, Q4 FY21 was another robu...  \n",
      "2478   DoorDash Inc., the largest food-delivery comp...  \n",
      "3182  IDBI Capital Report, Nestle India Ltd. reporte...  \n",
      "292   (Bloomberg Opinion) -- One of the issues compl...  \n",
      "386    Sony Interactive Entertainment LLC is operati...  \n",
      "4117   The University of Pennsylvania reported a 41....  \n",
      "104   (Bloomberg) -- Washington’s efforts to attract...  \n",
      "3310   Cox Communications Inc. said it would buy the...  \n",
      "2744   India’s Covid-19 crisis has pummeled demand f...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK resources (uncomment the following lines if not already downloaded)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load pre-trained GloVe vectors\n",
    "glove_model = KeyedVectors.load_word2vec_format('gloves/converted_vectors.txt', binary=False)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_word_vector(word, model):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_similarity_with_keyword(text, keyword, model):\n",
    "    tokens = preprocess_text(text)\n",
    "    keyword_vector = get_word_vector(keyword, model)\n",
    "    if keyword_vector is None:\n",
    "        return 0  # Keyword not found in the model\n",
    "    weighted_similarities = []\n",
    "    for token in tokens:\n",
    "        token_vector = get_word_vector(token, model)\n",
    "        if token_vector is not None:\n",
    "            similarity = cosine_similarity([keyword_vector], [token_vector])[0][0]\n",
    "            weighted_similarities.append(similarity)\n",
    "    if len(weighted_similarities) == 0:\n",
    "        return 0  # No valid vectors found in the text\n",
    "    return np.mean(weighted_similarities)\n",
    "\n",
    "def find_top_articles_with_keyword(data, keyword, weights={'title': 0.5, 'short_description': 0.3, 'description': 0.2}):\n",
    "    weighted_similarity_scores = []\n",
    "    for index, row in data.iterrows():\n",
    "        title_score = find_similarity_with_keyword(row['title'], keyword, glove_model) * weights['title']\n",
    "        short_desc_score = find_similarity_with_keyword(row['short_description'], keyword, glove_model) * weights['short_description']\n",
    "        desc_score = find_similarity_with_keyword(row['description'], keyword, glove_model) * weights['description']\n",
    "        total_score = title_score + short_desc_score + desc_score\n",
    "        weighted_similarity_scores.append((index, total_score))\n",
    "    weighted_similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_articles = weighted_similarity_scores[:10]\n",
    "    top_article_indices = [article[0] for article in top_articles]\n",
    "    return data.iloc[top_article_indices]\n",
    "\n",
    "# Sample usage:\n",
    "keyword = \"france\"  # Specify the keyword to search for\n",
    "top_articles = find_top_articles_with_keyword(data, keyword)\n",
    "print(top_articles[['title', 'short_description', 'description']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format('gloves/converted_vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  title  \\\n",
      "2400  Biden Says He Was Unaware of Giuliani Raid, Wo...   \n",
      "4117  Penn Endowment Posts 41% Return, Buoyed by Sto...   \n",
      "104   Washington and Boston Are Beating NYC for Entr...   \n",
      "3673  Cyient Q4 Review - Strong Show Continues: Prab...   \n",
      "3311  Biden Talks Up Benefits of Vaccines After New ...   \n",
      "1273  Reliance Jio Q4 Review - Weak Financial Perfor...   \n",
      "1186  California Tribe Buys Palms Casino in Vegas fo...   \n",
      "3142  Shopify Turns to ‘Harry Potter’ to Show Heft A...   \n",
      "1860  Apple Trial Threatens to Reveal App Store's Co...   \n",
      "3475  Glencore Chair Defends Pay Plan for Commodity ...   \n",
      "\n",
      "                                      short_description  \\\n",
      "2400  Biden Says He Was Unaware of Giuliani Raid, Wo...   \n",
      "4117  Penn Endowment Posts 41% Return, Buoyed by Sto...   \n",
      "104   Washington, Boston Beating NYC for Entrepreneu...   \n",
      "3673  Cyient Q4 Review - Strong Show Continues: Prab...   \n",
      "3311  However, masks should remain on anywhere there...   \n",
      "1273  Reliance Jio Q4 Review - Weak Financial Perfor...   \n",
      "1186  California Tribe to Buy Palms Casino in Vegas ...   \n",
      "3142  Shopify Touts Its Global Economic Impact Ahead...   \n",
      "1860  Apple Trial Threatens to Reveal App Store's Co...   \n",
      "3475  Glencore Chair Defends Pay Plan for Commodity ...   \n",
      "\n",
      "                                            description  \n",
      "2400   President Joe Biden said he didn’t know ahead...  \n",
      "4117   The University of Pennsylvania reported a 41....  \n",
      "104   (Bloomberg) -- Washington’s efforts to attract...  \n",
      "3673  Prabhudas Lilladher Report, Cyient Ltd. report...  \n",
      "3311   President Joe Biden urged Americans hesitant ...  \n",
      "1273  Dolat Capital Report, Reliance Jio’s Q4 FY21 n...  \n",
      "1186   The San Manuel Band of Mission Indians agreed...  \n",
      "3142   Shopify Inc. underpins a global business ecos...  \n",
      "1860   ,  App Store has long been touted as a growth...  \n",
      "3475   Glencore Plc’s chairman defended an incentive...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Download NLTK resources (uncomment the following lines if not already downloaded)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_word_vector(word, model):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_similarity_with_keyword(text_vectors, keyword_vector):\n",
    "    valid_text_vectors = [vec for vec in text_vectors if vec is not None]  # Filter out NaN values\n",
    "    if not valid_text_vectors:\n",
    "        return 0  # No valid vectors found in the text\n",
    "\n",
    "    similarity_scores = [cosine_similarity([keyword_vector], [vec])[0][0] for vec in valid_text_vectors]\n",
    "    return np.mean(similarity_scores)\n",
    "\n",
    "\n",
    "def find_top_articles_with_keyword(data, keyword, weights, user_preferences):\n",
    "    keyword_vector = get_word_vector(keyword, glove_model)\n",
    "    if keyword_vector is None:\n",
    "        return \"Keyword not found in the model\"\n",
    "    \n",
    "    similarity_scores = []\n",
    "    for index, row in data.iterrows():\n",
    "        title_tokens = preprocess_text(row['title'])\n",
    "        short_desc_tokens = preprocess_text(row['short_description'])\n",
    "        desc_tokens = preprocess_text(row['description'])\n",
    "\n",
    "        # Convert tokens into GloVe vectors\n",
    "        title_vectors = [get_word_vector(word, glove_model) for word in title_tokens]\n",
    "        short_desc_vectors = [get_word_vector(word, glove_model) for word in short_desc_tokens]\n",
    "        desc_vectors = [get_word_vector(word, glove_model) for word in desc_tokens]\n",
    "\n",
    "        # Calculate similarity scores for title, short description, and description\n",
    "        title_similarity = find_similarity_with_keyword(title_vectors, keyword_vector)\n",
    "        short_desc_similarity = find_similarity_with_keyword(short_desc_vectors, keyword_vector)\n",
    "        desc_similarity = find_similarity_with_keyword(desc_vectors, keyword_vector)\n",
    "\n",
    "        # Combine similarity scores with weights\n",
    "        total_similarity = (weights['title'] * title_similarity +\n",
    "                            weights['short_description'] * short_desc_similarity +\n",
    "                            weights['description'] * desc_similarity)\n",
    "        \n",
    "        # Apply user preferences weighting\n",
    "        topic_weight = user_preferences.get(row['category'], 0)\n",
    "        total_similarity *= topic_weight\n",
    "        \n",
    "        similarity_scores.append((index, total_similarity))\n",
    "    \n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_articles = similarity_scores[:10]\n",
    "    top_article_indices = [article[0] for article in top_articles]\n",
    "    return data.iloc[top_article_indices]\n",
    "\n",
    "# Load user preferences from JSON\n",
    "def load_user_preferences_from_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        user_data = json.load(f)\n",
    "    return user_data['user_preferences']\n",
    "\n",
    "# Sample usage:\n",
    "keyword = \"france\"  # Specify the keyword to search for\n",
    "weights = {'title': 0.5, 'short_description': 0.3, 'description': 0.2}\n",
    "\n",
    "# Load user preferences\n",
    "user_preferences = load_user_preferences_from_json('data/user.json')\n",
    "\n",
    "top_articles = find_top_articles_with_keyword(data, keyword, weights, user_preferences)\n",
    "print(top_articles[['title', 'short_description', 'description']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
